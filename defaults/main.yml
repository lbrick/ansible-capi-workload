---
kubernetes_version: v1.30.5
capi_image_name: rocky-9-containerd-v1.30.5 #ubuntu-2204-kube-v1.26.7-ca

capi_provider_version: v0.8.0

cluster_rdc_project: NeSI_RDC_PROJECT_NAME
cluster_name: CAPI_CLUSTER_NAME
cluster_namespace: "{{ cluster_rdc_project | lower }}"

capi_management_cluster: MANGEMENT_CLUSTER_NAME

openstack_ssh_key: NeSI_RDC_KEYPAIR_NAME

cluster_control_plane_count: 1
control_plane_flavor: balanced1.2cpu4ram
control_plane_volume_size: 0

cluster_worker_count: 2
cluster_max_worker_count: 3
worker_flavour: balanced1.2cpu4ram
worker_volume_size: 0

cluster_node_cidr: 10.10.0.0/24
cluster_pod_cidr: 192.168.0.0/16

cluster_external_network_id: 3f405cc9-28a3-4973-b5a1-7f50f112e5d5

cluster_network: "{{ cluster_rdc_project }}"

additional_cluster_networks: []

cluster_settle_timeout_base: 3

bin_dir: /usr/local/bin

tmp_dir: "/tmp/{{ cluster_name }}"

clouds_yaml_local_location: ~/.config/openstack/clouds.yaml
clouds_yaml_location: "{{ tmp_dir }}/clouds.yaml"
clouds_yaml_cloud: openstack

kube_config_local_location: ~/.kube/config
kube_config_location: "~/.kube"

yq_version: v4.2.0 
yq_repo: "https://github.com/mikefarah/yq/releases/download/{{ yq_version }}/yq_linux_amd64"

helm_cli_repo: "https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3"

ca_cert_dir: /usr/local/share/ca-certificates
ca_cert_file: "{{ ca_cert_dir }}/zero_ca.crt"

autoscaler_ns: "{{ cluster_rdc_project | lower }}"
autoscaler_image: "registry.k8s.io/autoscaling/cluster-autoscaler:v1.32.2"
autoscaler_overprovision: false

cloud_provider_openstack_version: v1.33.0

#cri_socket: /var/run/crio/crio.sock
cri_socket: ""

# CNI settings

# can either be calico, calico-operator, cilium (not implmented yet)
cni_provider: calico

# GPU Settings

enable_gpu_nodes: false

cluster_gpu_worker_count: 1
gpu_worker_flavour: gpu1.40cpu200ram.a40.1g.48gb

gpu_kubernetes_version: v1.33.3
capi_gpu_image_name: rocky-9-containerd-hpc-nvidia-v1.33.3

# can either be nvidia, hami (Currently hami is still in development)
gpu_operator: nvidia

## NVIDIA GPU Operator settings
## https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/overview.html

## https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/platform-support.html#gpu-operator-component-matrix
nvidia_gpu_operator_version: 25.3.1
nvidia_container_toolkit_version: 1.17.8

## options are mps or time-slicing
## https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/gpu-sharing.html
nvidia_gpu_sharing_type: mps

# Settings related to managing the security groups with Ansible
# Changing this to default to false as we generally want to create and manage our own security groups
capi_managed_secgroups: false

additional_cluster_secgroups: []

yaml_openstack_cloud: openstack

source_ips:
  - 163.7.144.0/21

## Variables for OpenID Connect Configuration https://kubernetes.io/docs/admin/authentication/
## To use OpenID you have to deploy additional an OpenID Provider (e.g Dex, Keycloak, ...)

kube_oidc_auth: false
kube_oidc_hostname: ood-idp.training.data.nesi.org.nz
kube_oidc_url: https://{{ kube_oidc_hostname }}/realms/ondemand
kube_oidc_client_id: kubernetes
## Optional settings for OIDC
kube_oidc_username_claim: preferred_username
kube_oidc_username_prefix: ""
kube_oidc_groups_claim: groups
kube_oidc_groups_prefix: ""
# Copy oidc CA file to the following path if needed
kube_oidc_ca_file: "{{ ca_cert_file }}"

preKubeadmCommands: []

postKubeadmCommands: []


## Templating options

generate_templates: true
fluxcd_deployment: true

templates_dir: /tmp/{{ cluster_name }}-templates

cluster_repo_dir: "{{ templates_dir }}/cluster-repo"
flux_repo_dir: "{{ templates_dir }}/flux-repo"

cluster_definition: "{{ cluster_repo_dir }}/cluster-definition"
terraform_templates: "{{ cluster_repo_dir }}/terraform"

cluster_infra: "{{ cluster_repo_dir }}/cluster-infra"
cluster_support_apps: "{{ cluster_repo_dir }}/cluster-services"

cluster_definition_repo: https://gitlab.com/nesi1/flexihpc/cluster-definition-capi
flux_cluster_auth_secret: flux-deploy-authentication